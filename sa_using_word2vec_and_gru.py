# -*- coding: utf-8 -*-
"""SA using word2vec and gru.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZtjAIgSajfZ_GeJT5iTm2ZcNza3L3VtZ
"""

import numpy as np
import pandas as pd
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, GRU, Embedding, Flatten
from keras.layers.embeddings import Embedding
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import *
import nltk
import re
from bs4 import BeautifulSoup
import gensim
from nltk.tokenize import word_tokenize
import string
from keras.initializers import Constant
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

df=pd.DataFrame()
df=pd.read_csv('IMDB Dataset.csv',encoding='utf-8')

nltk.download('punkt')
nltk.download('stopwords')

def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Removing the square brackets
def remove_between_square_brackets(text):
    return re.sub('\[[^]]*\]', '', text)

#Removing the noisy text
def denoise_text(text):
    text = strip_html(text)
    text = remove_between_square_brackets(text)
    return text

df['review']=df['review'].apply(denoise_text)
review_lines=list()
lines=df['review'].values.tolist()
for line in lines:
  tokens=word_tokenize(line)
  tokens=[word.lower() for word in tokens]
  #table=str.maketrans('','',string.punctuation)
  stripped=[re.sub(r'[^a-zA-Z#]','',w) for w in tokens]
  words=[w for w in stripped if w.isalpha()]
  stop_words=set(stopwords.words('english'))
  words=[w for w in words if w not in stop_words]
  review_lines.append(words)



model=gensim.models.Word2Vec(sentences=review_lines,size=100,window=5,workers=4,min_count=1)
words=list(model.wv.vocab)
print("Vocabulary Size of word to vec %d" % (len(words)))

#save_model
filename="imdb_word2vec.txt"
model.wv.save_word2vec_format(filename,binary=False)

import os
embeddings_index={}
f=open(os.path.join('',filename))
for itr in f:
  values=itr.split()
  word=values[0]
  coefs=np.asarray(values[1:])
  embeddings_index[word]=coefs
  
    
    
f.close()

def IMDB_sentiment_label_changer(label):
  if label=="positive":
    label=0
  elif (label=='negative'):
    label=1
  return label

#converting data to a 2d tensor
tokenize_obj=Tokenizer()
tokenize_obj.fit_on_texts(review_lines)
sequences=tokenize_obj.texts_to_sequences(review_lines)

max_length=100
review_pad=pad_sequences(sequences,maxlen=max_length)
df['sentiment']=df['sentiment'].apply(IMDB_sentiment_label_changer)
sentiment_pad=df['sentiment'].values

#print ("Shape of Review tensor %d" % (review_pad.shape))
#print ("Shape of Sentiment tensor %d" % (sentiment_pad.shape))

num_words=len(tokenize_obj.word_index)+1
embedding_matrix=np.zeros((num_words,100))

for  word, i in tokenize_obj.word_index.items():
  if i>num_words:
    continue
  embedding_vector=embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i]=embedding_vector

np.unique(embedding_matrix)

indices=np.arange(review_pad.shape[0])
np.random.shuffle(indices)
review_pad=review_pad[indices]
sentiment_pad=sentiment_pad[indices]
num_validation_steps=int(0.2*review_pad.shape[0])
X_train_pad=review_pad[:-num_validation_steps]
Y_train=sentiment_pad[:-num_validation_steps]
X_test_pad=review_pad[-num_validation_steps:]
Y_test=sentiment_pad[-num_validation_steps:]

EMBEDDING_DIM=100
print('Build model...')
vocab_size=len(tokenize_obj.word_index)+1
model=Sequential()

model.add(Embedding(vocab_size,EMBEDDING_DIM,embeddings_initializer=Constant(embedding_matrix),input_length=max_length,trainable=False))
model.add(GRU(units=32,dropout=0.2,recurrent_dropout=0.2))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model

model.fit(X_train_pad,Y_train,batch_size=128,epochs=25,validation_data=(X_test_pad,Y_test),verbose=2)

loss, accuracy=model.evaluate(X_test_pad,Y_test,batch_size=128)
print("Accuracy %d" %(100*accuracy))

sample_1="Overall the movie was nice"
sample_2="It was a mix of good and bad movie"
sample=[sample_1,sample_2]

seq=tokenize_obj.texts_to_sequences(sample)
test=pad_sequences(seq,maxlen=100)

model.predict(test)

.